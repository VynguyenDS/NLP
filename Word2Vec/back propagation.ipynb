{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Step\n",
    "# Initialize Network\n",
    "# Forward Propagete.\n",
    "# Back Propagate Error,\n",
    "# Train Network.\n",
    "# Predict\n",
    "# Seed Datasets Case Study\n",
    "\n",
    "\n",
    "# These steps will provide the foundation that you need to implement the backpropagation algorithm \n",
    "# from scratch and apply it to your own predictive modeling problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize Network\n",
    "# Each neuron has a set of weights that need to be maintained\n",
    "# One weight for each input connection and an additional weight for the bias.\n",
    "# A network is organized into layers.\n",
    "# The first real layer is the hidden layer. \n",
    "# This is followed by the output layer that has one neuron for each class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_network() that creates a new neural network ready for training.\n",
    "# It accepts three parameters, the number of inputs, \n",
    "# the number of neurons to have in the hidden layer and the number of outputs.\n",
    "\n",
    "# n_hidden neurons and each neuron in the hidden layer has n_inputs + 1 weights, \n",
    "# one for each input column in a dataset and an additional one for the bias.\n",
    "\n",
    "# the output layer that connects to the hidden layer has n_outputs neurons, each with n_hidden + 1 weights.\n",
    "# each neuron in the output layer connects to (has a weight for) each neuron in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
      "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from random import seed\n",
    "from random import random\n",
    " \n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    " \n",
    "seed(1)\n",
    "network = initialize_network(2, 1, 2)\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6629970129852887, 0.7253160725279748]\n"
     ]
    }
   ],
   "source": [
    "# Forward Propagate\n",
    "# We can calculate an output from a neural network by propagating an input signal through\n",
    "# each layer until the output layer outputs it's values\n",
    "# Calculate neuron activation for an input\n",
    "# activate mean linear regresion: y(activation) = ax +b depends on the dataset you have\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights) - 1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "# like sigmoid, which probability each outcome have \n",
    "# 1/(1+ e^ activation)\n",
    "def transfer(activation):\n",
    "    return 1.0/(1.0+ np.exp(-activation))\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
    "           [{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
    "row = [1, 0, None]\n",
    "output = forward_propagate(network, row)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "# Back Propagate Error\n",
    "# The backpropagation algorithm is named for the way in which weights are trained.\n",
    "# Error is calculated between the expected outputs and the outputs forward propagated from the network\n",
    "# These errors are then propagated backward through the network from the output layer to the hidden layer, \n",
    "# assigning blame for the error and updating weights as they go.\n",
    "# This part is broken down into two sections.\n",
    "    # Transfer Derivative.\n",
    "    # Error Backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Transfer Derivative\n",
    "# Given an output value from a neuron, we need to calculate itâ€™s slope.\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'delta': -0.0005348048046610517}]\n",
      "[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'delta': -0.14619064683582808}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763], 'delta': 0.0771723774346327}]\n"
     ]
    }
   ],
   "source": [
    "# 3.2. Error Backpropagation\n",
    "\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
    "[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
    "expected = [0, 1]\n",
    "backward_propagate_error(network, expected)\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network is trained using stochastic gradient descent.\n",
    "# backpropagating the error and updating the network weights.\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=6.819\n",
      ">epoch=1, lrate=0.500, error=5.917\n",
      ">epoch=2, lrate=0.500, error=5.372\n",
      ">epoch=3, lrate=0.500, error=5.251\n",
      ">epoch=4, lrate=0.500, error=5.240\n",
      ">epoch=5, lrate=0.500, error=5.213\n",
      ">epoch=6, lrate=0.500, error=5.148\n",
      ">epoch=7, lrate=0.500, error=5.038\n",
      ">epoch=8, lrate=0.500, error=4.885\n",
      ">epoch=9, lrate=0.500, error=4.695\n",
      ">epoch=10, lrate=0.500, error=4.468\n",
      ">epoch=11, lrate=0.500, error=4.207\n",
      ">epoch=12, lrate=0.500, error=3.921\n",
      ">epoch=13, lrate=0.500, error=3.622\n",
      ">epoch=14, lrate=0.500, error=3.324\n",
      ">epoch=15, lrate=0.500, error=3.035\n",
      ">epoch=16, lrate=0.500, error=2.764\n",
      ">epoch=17, lrate=0.500, error=2.515\n",
      ">epoch=18, lrate=0.500, error=2.289\n",
      ">epoch=19, lrate=0.500, error=2.086\n",
      "[{'weights': [1.2508178628624307, -1.70839446417483, -0.3991177297838095], 'output': 0.9446393078263786, 'delta': 0.010205776833704739}, {'weights': [0.8443968750118955, 0.4928715923394345, 0.2093969687445748], 'output': 0.9997793238185477, 'delta': -6.357268489583483e-06}]\n",
      "[{'weights': [-2.0121135616933223, 0.27494541533825523, 0.495066004936168], 'output': 0.25737850332024625, 'delta': -0.04919399116261438}, {'weights': [1.9672402611799285, -0.2561645963418584, -0.4845951784941402], 'output': 0.7398582537137796, 'delta': 0.05006896633938328}]\n"
     ]
    }
   ],
   "source": [
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "# how many output categorical\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "n_outputs\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "network\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "for layer in network:\n",
    "\tprint(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "for row in dataset:\n",
    "\tprediction = predict(network, row)\n",
    "\tprint('Expected=%d, Got=%d' % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What Is an Epoch?\n",
    "\n",
    "# The number of epochs is a hyperparameter that defines the number times that \n",
    "# the learning algorithm will work through the entire training dataset.\n",
    "\n",
    "# One epoch means that each sample in the training dataset has had an opportunity \n",
    "# to update the internal model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back Propagation and Weight Updation like update weight to make the output must to minimal errors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
